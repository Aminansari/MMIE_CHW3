{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b13f6a",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Mathematical Methods In Engineering - 25872</h1>\n",
    "<h4 align=\"center\">Dr. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2023</h4>\n",
    "<h4 align=\"center\">Python Assignment 3</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f25aa",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9196a1",
   "metadata": {},
   "source": [
    "**Feel free to contact us via telegram if you have any question: @SAHABE200 (Q2) ,@amirsoleix (Q1 and Q3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fecb98",
   "metadata": {},
   "source": [
    "# Q1: SVD for Compression (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a88c27",
   "metadata": {},
   "source": [
    "One application of Singular Value Decomposition is lossy image compression. In this question, we want to compress BMP images using SVD. BMP images are not compressed by default and the pixel information is stored in rather a simple format. Colored BMP files are composed of three matrices of size $n × m$ where $n$ and $m$ are the height and width of the image, respectively. Each matrix corresponds to one of the three colors Red, Green, and Blue. Each entry of these matrices is a number between 0 and 255 that specifies the intensity of the corresponding color. The final array is hence a 3D array of size $n × m × 3$.  \n",
    "To open BMP files in Python, you can use the `imread` function from `matplotlib.image` library and to show the image, you can use the `imshow` function from `matplotlib.pyplot` library.\n",
    "\n",
    "### 1.1 Image display  (2.5 points)\n",
    "Choose two arbitrary BMP files (it is recommended to use different subjects and sizes) and load it into a `numpy` array using the `imread` function. Then, show each image.  \n",
    "Apply the following steps to each of the two images and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2490ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c402ae",
   "metadata": {},
   "source": [
    "### 1.2 Channel Separation (2.5 points)\n",
    "Separate three channels corresponding to red, green, and blue colors.\n",
    "``` Python\n",
    "r = img[:, :, 0]\n",
    "g = img[:, :, 1]\n",
    "b = img[:, :, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf2cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43fc7a11",
   "metadata": {},
   "source": [
    "### 1.3 Singular Value Decomposition (10 points)\n",
    "Using appropriate libraries, calculate the SVD decomposition of each of the arrays.\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "where $A$ is the original array, $U$ is an $n × n$ unitary matrix, $\\Sigma$ is an $n × m$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $V$ is an $m × m$ unitary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08b59aa6",
   "metadata": {},
   "source": [
    "### 1.4 Compression (10 points)\n",
    "Since eigenvalues in the decomposition are sorted in descending order, we can approximate the original array by keeping only the first $k$ columns of $U$, first $k$ rows and columns of $\\Sigma$, and first $k$ rows of $V^T$.  \n",
    "Select $k$ to be 5, 10, 50, 100 and 250 and approximate the original array using the truncated SVD. Show the approximated image titled with the value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b1ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d9c5e6",
   "metadata": {},
   "source": [
    "### 1.5 Compression Factor (10 points)\n",
    "It seems that no compression is done since the size of the approximated array is the same as the original array. However, there is no need to store $U$ and $V$ matrices. Instead, we can store $\\Sigma$ and two other matrices of size $n × k$ and $m × k$. Hence, we can compress the image by a factor of $\\frac{nm}{k(n+m+1)}$. Calculate the compression factor for each value of $k$ and display the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d6724f",
   "metadata": {},
   "source": [
    "# Q2: PCA in Image Processing (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b6cf4",
   "metadata": {},
   "source": [
    "In this question you will implement the PCA from scratch and apply it for dimensionality\n",
    "reduction and image denoising. you will use the well-known MNIST data set that is commonly used for training various image processing systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ab4c8",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation (5 points)\n",
    "The MNIST data set is a large data set of handwritten digits (from\n",
    "0 to 9), containing 60000 gray-scale images for training and 10000 for testing, each image has 28x28 pixels with range of possible values from 0 to 255.\n",
    "\n",
    "we will consider a small sample of MNIST data set with size of 2000.\n",
    "\n",
    "Re-scale the images to [0, 1] dividing them by 255. Vectorize each image xi ∈ $R^d$ and form a matrix X=$[x1, . . . , xn]^T$ ∈ $R_{n×d}$.\n",
    "Remark that we will have d=784 and n=2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from numpy.linalg import eigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the data set(use just x_train for next part)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42637fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: selecting 2000 images and dividing them by 255\n",
    "\n",
    "# TODO: vectorize each image and forming matrix X\n",
    "\n",
    "# TODO: normalizing the data mean=0 std=1(hint:use StandardScaler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7211ef",
   "metadata": {},
   "source": [
    "#### 2. Compute the Eigendecomposition (10 points)\n",
    "Compute the eigendecomposition of the sample covariance matrix and use the eigenvalues to calculate the percentage of variance explained (given by the eigenvalues).\n",
    "Plot the cumulative sum of these percentages (also known as cumulative explained variance. you can read about it in [cumulative explained variance](https://vitalflux.com/pca-explained-variance-concept-python-example/)) versus the number of components.\n",
    "\n",
    "*Hint: you can use cumsum from Numpy to calculate the cumulative sum.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7d608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the covariance matrix\n",
    "\n",
    "\n",
    "# TODO: Eigendecomposition of covariance matrix\n",
    "\n",
    "\n",
    "# TODO: Compute explained variance ---> eigenvalue/(sum of egn_values )\n",
    "\n",
    "\n",
    "# TODO: Plot the cumulative explained variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23ca3e",
   "metadata": {},
   "source": [
    "#### 3. Reduce the Dimensionality (10 points)\n",
    "Apply the PCA via Eigendecomposition to reduce the dimensionality of the images for each p ∈ {50, 250, 500}.\n",
    "\n",
    "Compute the normalized reconstruction error in terms of the Frobenius norm, i.e. $e_p = \\frac{||X - {X_p}||_F}{||X||_F}$ , where $X$ denotes the input matrix, and ${X_p}$ denotes the recovered matrix associated to each p. \n",
    "\n",
    "Visualize some recovered images and compare them with their corresponding original images.\n",
    "\n",
    "what happens when we reduce the number of components p?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6af12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the PCA\n",
    "\n",
    "# TODO: reconstruct the orginal matrix X\n",
    "\n",
    "# TODO: Visualize recovered images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288fa707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the normalized reconstruction error\n",
    "\n",
    "# TODO: Plot the normalized reconstruction error vs number of principal components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fd00f",
   "metadata": {},
   "source": [
    "#### 4. Image noising (5 points)\n",
    "Considering the same input matrix, let’s add some Gaussian noise (make sure that the range of possible values of the generated noisy data will remain between 0 and1, as well as the input data) with zero mean and variance $σ^2$ = 0.25.\n",
    "\n",
    "Visualize the corrupted images and compare them with their corresponding original images versus the number of components, as in the first item. \n",
    "\n",
    "Compare it with the one obtained in the noiseless case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88bb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adding Gaussian noise\n",
    "\n",
    "# TODO: Visualize the corrupted images\n",
    "\n",
    "# TODO: Comparing with noiseless case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85613092",
   "metadata": {},
   "source": [
    "#### 5. Recovering Images (10 points)\n",
    "Now we will apply the PCA for image denoising. Generate the noisy data, for each $σ^2$ ∈ {0.15, 0.25, 0.50}.\n",
    "Apply the PCA via Eigendecomposition for each $σ^2$ and fixing p = 250.\n",
    "\n",
    "Visualize some recovered images and compare them with their corresponding noisy images.\n",
    "\n",
    "Compute the normalized reconstruction error in terms of the Frobenius norm, obtained for all values of $σ^2$, with respect to the original images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39d10f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generating noisy data\n",
    "\n",
    "# TODO: Apply PCA via Eigendecomposition\n",
    "\n",
    "# TODO: Visualize recovered images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d61f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the normalized reconstruction\n",
    "\n",
    "# TODO: plot the normalized reconstruction error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae349a",
   "metadata": {},
   "source": [
    "# Q3: PCA in Data Analysis (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72334e47",
   "metadata": {},
   "source": [
    "We have a dataset containing the movie ratings from 1,000 users. There are 200 movies in the dataset, and each user has rated all of them. The ratings are integers from 1 (worst) to 5 (best). The goal of this problem set is to perform PCA on this dataset and analyze the results.  \n",
    "### 3.1: Data Loading and Visualization (5 points)\n",
    "First, we need to load the dataset and understand its structure. This data is stored in a CSV file named ratings.csv, with 1,000 rows and 200 columns. Each row corresponds to a user, and each column corresponds to a movie.\n",
    "\n",
    "1. Load the CSV file into a NumPy array or a pandas DataFrame.\n",
    "2. Visualize the first few rows of the data to understand its structure.\n",
    "3. Plot a histogram to show the distribution of the ratings in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca4125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de2e8b5",
   "metadata": {},
   "source": [
    "### 3.2 Data Preprocessing (5 points)\n",
    "Before applying PCA, we should preprocess the data.\n",
    "\n",
    "1. Normalize the data so that each movie has a mean rating of 0 and a standard deviation of 1. This ensures that our PCA isn't affected by the scale of the ratings.\n",
    "2. Split the dataset into a training set and a test set. Use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead19763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba7d733",
   "metadata": {},
   "source": [
    "### 3.3 Principal Component Analysis (10 points)\n",
    "Now we can apply PCA to the training set.\n",
    "\n",
    "1. Implement PCA. You might want to use a library like scikit-learn, or you might choose to implement it from scratch.  \n",
    "**Note: Implementation from scratch is bonus.**\n",
    "2. Choose the number of principal components to retain. Explain why you chose this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec62b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0db982",
   "metadata": {},
   "source": [
    "### 3.4 Interpretation of Principal Components (10 points)\n",
    "The principal components can be difficult to interpret. But since our dataset is about movie ratings, we might expect that they correspond to some kinds of `tastes` in movies.\n",
    "\n",
    "For each of the first few principal components, find the 5 movies with the highest and lowest loadings.  \n",
    "`Loading` refers to the coefficients of the linear combination of the original variables (in your case, movie ratings) that make up a principal component. They reflect how much each original variable contributes to the principal component.  \n",
    "In terms of their `highest` and `lowest` loading:\n",
    "1. The variables (or in the context, movies) with the highest loadings on a principal component are those that contribute the most (positively) to that component. In other words, these are the movies that a user rates similarly when considering this underlying factor. They might represent a specific taste in movies. For example, if the first principal component has high loadings for a lot of action movies, it might represent a taste for action films.\n",
    "2. Conversely, the variables with the lowest loadings (or the most negative) on a principal component contribute negatively to that component. In the context of movie ratings, these might represent the movies that a user tends to rate lower when this underlying taste is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3eb614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba41bf41",
   "metadata": {},
   "source": [
    "### 3.5 Reconstruction of Ratings and Error Analysis (15 points)\n",
    "Now we will use our PCA to make predictions.\n",
    "\n",
    "1. **Project the test set onto the principal components, then reconstruct it.**  \n",
    "  After you've found the principal components (from your training set), you can use them to transform your test set. This involves representing each user in the test set as a combination of the principal components. Then, you `reconstruct` the test set. This means you convert your data back from the principal component space to the original space of movie ratings. This gives you a set of `predicted` ratings for each movie. These predicted ratings aren't exactly the original ratings. They're what the PCA model thinks the ratings should be, based on the patterns it learned from the training set.\n",
    "2. **Measure the mean squared error (MSE) between the original ratings and the reconstructed ratings. Is the error high or low? Why?**  \n",
    "  To evaluate how good your PCA model is, you compare the predicted ratings to the actual ratings. You calculate the Mean Squared Error (MSE), which is a common way to measure the difference between predicted and actual values.\n",
    "3. **Plot a scatterplot to show the original ratings (on the x-axis) and the predicted ratings (on the y-axis) for a few users and movies.**  \n",
    " Finally, you visualize the results with a scatterplot. The x-coordinate is the original rating, and the y-coordinate is the predicted rating. If the PCA model is perfect, all points would lie along the line y = x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f872c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93fd8cdf",
   "metadata": {},
   "source": [
    "Please submit your Python notebook with all code, outputs, and explanations clearly shown. We'll evaluate your problem set based on the correctness and clarity of your code, the correctness and depth of your explanations, and the creativity of your interpretations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
